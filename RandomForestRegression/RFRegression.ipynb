{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "252317a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d0c50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b8da9783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 15)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Furniture.csv', skiprows = 1, header = None)\n",
    "data.head(10)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2b47afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_each_tree(data, num_trees=10):\n",
    "    total_columns = np.arange(data.shape[1] - 1)\n",
    "    last_column = data.shape[1] - 1\n",
    "    random_datasets = []\n",
    "    for _ in range(num_trees):\n",
    "        # Select random columns without replacement\n",
    "        random_columns = np.random.choice(total_columns, 4, replace=False)\n",
    "        subset_data = data[:, random_columns]\n",
    "        # Add target column (last column)\n",
    "        subset_with_target = np.concatenate([subset_data, data[:, last_column].reshape(-1, 1)], axis=1)\n",
    "        random_datasets.append(subset_with_target)\n",
    "\n",
    "    return random_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54efee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index = None, threshold = None, left = None, right = None,info_gain = None, value = None):\n",
    "        ## for decision nodes\n",
    "        self.feature_index = feature_index\n",
    "        ## for example if the feature is sepal_length feature_index = 0\n",
    "        self.threshold = threshold\n",
    "        ## for example if the feature is compared to 3 threshol = 3 \n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        ## for leaf nodes\n",
    "        self.value = value ## majority class of the leaf(the class of the new data point if fits in this node)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8eba2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split = 10, max_depth = 10):\n",
    "        self.root = None\n",
    "        \n",
    "        ## stopping condition\n",
    "        self.min_samples_split = min_samples_split\n",
    "        ## if in a node the number of samples < min_samples_split we stop\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        X,Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split[\"info_gain\"] > 0: ## if info_gain is 0 -> all values inside the node are of the same class\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth +1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"],curr_depth+1)\n",
    "                return Node(best_split[\"feature_index\"],best_split[\"threshold\"],\n",
    "                            left_subtree,right_subtree,best_split[\"info_gain\"])\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value = leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset,feature_index, threshold)\n",
    "                \n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    y, left_y, right_y = dataset[:, -1],dataset_left[:,-1], dataset_right[:,-1]\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self,  parent, l_child,r_child, mode = \"entropy\"):\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode == \"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l* self.gini_index(l_child) + weight_r * self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weght_l* self.entropy(l_child) + weight_r * self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self,y):\n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) /len(y)\n",
    "            gini += p_cls *p_cls\n",
    "        return 1 - gini\n",
    "    \n",
    "    def calculate_leaf_value(self,Y):\n",
    "        Y = list(Y)\n",
    "        sum_of_nodes = 0\n",
    "        for y in Y:\n",
    "            sum_of_nodes += y\n",
    "        return sum_of_nodes / len(Y)    \n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "            \n",
    "    def fit(self,dataset):\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        predictions = [ self.make_prediction(x,self.root) for x in X]\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value != None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a65cec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, no_trees=10):\n",
    "        self.no_trees = no_trees\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        global_dataset = np.concatenate((X, Y), axis=1)\n",
    "        datasets_for_each_tree = create_data_for_each_tree(global_dataset, num_trees=self.no_trees)\n",
    "        for i in range(self.no_trees):\n",
    "            tree = DecisionTreeRegressor(min_samples_split=10, max_depth=5)\n",
    "            tree.fit(datasets_for_each_tree[i])\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def make_prediction(self, x):\n",
    "     
    "        predictions = Parallel(n_jobs=-1)(delayed(tree.make_prediction)(x, tree.root) for tree in self.trees)\n",
    "        return np.mean(predictions)  # Average predictions from all trees\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = Parallel(n_jobs=-1)(delayed(self.make_prediction)(x) for x in X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "27df1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1].values\n",
    "Y = data.iloc[:,-1].values.reshape(-1, 1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = .2, random_state = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2369d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 34 128  81 ...  10  85  40]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "Y_train_flat = Y_train.ravel()\n",
    "\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train_flat)\n",
    "Y_train_encoded_reshaped = Y_train_encoded.reshape(-1, 1)\n",
    "print(Y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7260c866",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m regressor \u001b[38;5;241m=\u001b[39m RandomForest(no_trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dataset_for_print \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_train,Y_train), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m random_datasets \u001b[38;5;241m=\u001b[39m create_data_for_each_tree(dataset_for_print)\n",
      "Cell \u001b[0;32mIn[97], line 13\u001b[0m, in \u001b[0;36mRandomForest.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_trees):\n\u001b[1;32m     12\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_for_each_tree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees\u001b[38;5;241m.\u001b[39mappend(tree)\n",
      "Cell \u001b[0;32mIn[96], line 100\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,dataset):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 18\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.build_tree\u001b[0;34m(self, dataset, curr_depth)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m## if info_gain is 0 -> all values inside the node are of the same class\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         left_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tree(best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_left\u001b[39m\u001b[38;5;124m\"\u001b[39m], curr_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m         right_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_split\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_right\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurr_depth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m\"\u001b[39m],best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m                     left_subtree,right_subtree,best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m leaf_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_leaf_value(Y)\n",
      "Cell \u001b[0;32mIn[96], line 18\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.build_tree\u001b[0;34m(self, dataset, curr_depth)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m## if info_gain is 0 -> all values inside the node are of the same class\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         left_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tree(best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_left\u001b[39m\u001b[38;5;124m\"\u001b[39m], curr_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m         right_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_split\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_right\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurr_depth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m\"\u001b[39m],best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m                     left_subtree,right_subtree,best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m leaf_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_leaf_value(Y)\n",
      "Cell \u001b[0;32mIn[96], line 18\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.build_tree\u001b[0;34m(self, dataset, curr_depth)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m## if info_gain is 0 -> all values inside the node are of the same class\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         left_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tree(best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_left\u001b[39m\u001b[38;5;124m\"\u001b[39m], curr_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m         right_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_split\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_right\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurr_depth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m\"\u001b[39m],best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m                     left_subtree,right_subtree,best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m leaf_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_leaf_value(Y)\n",
      "Cell \u001b[0;32mIn[96], line 15\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.build_tree\u001b[0;34m(self, dataset, curr_depth)\u001b[0m\n\u001b[1;32m     12\u001b[0m num_samples, num_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(X)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_samples \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split \u001b[38;5;129;01mand\u001b[39;00m curr_depth \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth:\n\u001b[0;32m---> 15\u001b[0m     best_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m## if info_gain is 0 -> all values inside the node are of the same class\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         left_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tree(best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_left\u001b[39m\u001b[38;5;124m\"\u001b[39m], curr_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[96], line 36\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.get_best_split\u001b[0;34m(self, dataset, num_samples, num_features)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_left) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_right) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m     y, left_y, right_y \u001b[38;5;241m=\u001b[39m dataset[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],dataset_left[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dataset_right[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 36\u001b[0m     curr_info_gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minformation_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_info_gain \u001b[38;5;241m>\u001b[39m max_info_gain:\n\u001b[1;32m     38\u001b[0m         best_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m feature_index\n",
      "Cell \u001b[0;32mIn[96], line 55\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.information_gain\u001b[0;34m(self, parent, l_child, r_child, mode)\u001b[0m\n\u001b[1;32m     53\u001b[0m weight_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(r_child) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(parent)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgini_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m (weight_l\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgini_index(l_child) \u001b[38;5;241m+\u001b[39m weight_r \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgini_index(r_child))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentropy(parent) \u001b[38;5;241m-\u001b[39m (weght_l\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentropy(l_child) \u001b[38;5;241m+\u001b[39m weight_r \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentropy(r_child))\n",
      "Cell \u001b[0;32mIn[96], line 72\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.gini_index\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m gini \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m class_labels:\n\u001b[0;32m---> 72\u001b[0m     p_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y[\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     73\u001b[0m     gini \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_cls \u001b[38;5;241m*\u001b[39mp_cls\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m gini\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor = RandomForest(no_trees = 10)\n",
    "regressor.fit(X_train, Y_train)\n",
    "dataset_for_print = np.concatenate((X_train,Y_train), axis = 1)\n",
    "random_datasets = create_data_for_each_tree(dataset_for_print)\n",
    "random_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = classifier.predict(X_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
